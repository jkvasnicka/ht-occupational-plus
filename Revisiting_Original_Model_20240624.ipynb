{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275e1922-cbd1-4f08-97e4-6d4bc4681afa",
   "metadata": {},
   "source": [
    "# Revisiting and Enhancing the Original Model for Predicting Workplace Air Concentrations\n",
    "\n",
    "**Date:** June 25, 2024 <br>\n",
    "**Author:** Jacob Kvasnicka\n",
    "\n",
    "### Recap Previous Discussion\n",
    "In our previous meeting, we outlined a three-strage framework for prioritizing chemicals of concern in workplaces:\n",
    "- Predict industrial sectors from chemical structure\n",
    "- Predict inhalation exposure using an enhanced version of Jeff's Bayesian model\n",
    "- Prioritize chemicals of concern, incorporating human-equivalent points of departure\n",
    "\n",
    "### Initial Objective \n",
    "The initial objective of this notebook was to replicate the original methodology for predicting air concentrations of chemicals using OSHA data. The goal was to determine whether similar performance could be attained using a model that's less computationally intensive than the Bayesian hierarchical model.\n",
    "\n",
    "As an alternative to the Bayesian model, I began to implement a Random Forests approach, maintaining the exact training and test sets from the original study:\n",
    "1. Classify whether a sample has a detectable air concentration\n",
    "2. Predict the concentration levels for those detectable samples\n",
    "\n",
    "### Issues Identified in Data Preparation\n",
    "However, further inspection of the original code and dataset preparation revealed potential issues that may have influenced the performance estimates:\n",
    "- **Duplicate Samples:** A considerable number of duplicate samples were found, attributable to errors in the data preparation process.\n",
    "- **Identically-Valued Samples:** A large proportion of the classification training samples are identical, which could potentially bias the model towards these repeated values.\n",
    "- **Data Leakage:** Preprocessing steps such as centering and scaling were conducted on the entire dataset prior to the train-test split, likely causing data leakage and potentially overoptimistic performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1faf63-71cb-4bf5-9015-a233dfefd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82727bd-257d-4e11-a1a8-2304092acc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmank\\AppData\\Local\\Temp\\ipykernel_18508\\2868573236.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  osha_full = pd.read_csv('../ht_occupational-1.0.0/output/osha_processed_full.csv')\n",
      "C:\\Users\\jmank\\AppData\\Local\\Temp\\ipykernel_18508\\2868573236.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  osha_train = pd.read_csv('../ht_occupational-1.0.0/output/osha_processed_training.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load data files\n",
    "osha_full = pd.read_csv('../ht_occupational-1.0.0/output/osha_processed_full.csv')\n",
    "osha_train = pd.read_csv('../ht_occupational-1.0.0/output/osha_processed_training.csv')\n",
    "osha_test = pd.read_csv('../ht_occupational-1.0.0/output/osha_processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef3d3a7-bd5e-4dc9-a9ae-4180dbb20d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data configuration settings\n",
    "data_config = {\n",
    "    'phys_chem_cols' : [\n",
    "        'logp_pred', \n",
    "        'bp_pred', \n",
    "        'loghl_pred', \n",
    "        'rt_pred', \n",
    "        'logoh_pred', \n",
    "        'logkoc_pred'\n",
    "    ],\n",
    "    'naics_col' : 'naics_unified',\n",
    "    'class_target_col' : 'detected',\n",
    "    'reg_target_col' : 'conc_mgm3',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e42c83e-a423-4844-ba7e-60147d1657db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_classification_data(\n",
    "        osha_data, \n",
    "        phys_chem_cols, \n",
    "        naics_col, \n",
    "        target_col\n",
    "    ):\n",
    "    '''\n",
    "    Prepare binary classification data.\n",
    "\n",
    "    The features include physical-chemical properties plus NAICS subsector \n",
    "    codes. The target is the sample detection category (detect/non-detect).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    osha_data : pd.DataFrame\n",
    "        The OSHA dataset, which may be the train or test set.\n",
    "    phys_chem_cols : list\n",
    "        List of columns representing physical-chemical properties.\n",
    "    naics_col : str\n",
    "        Column name representing NAICS code.\n",
    "    target_col : str\n",
    "        Column name representing the target variable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Features and target data.\n",
    "    '''\n",
    "    X = prepare_features(osha_data, phys_chem_cols, naics_col)\n",
    "    y = osha_data[target_col]\n",
    "\n",
    "    return X, y\n",
    "        \n",
    "def prepare_regression_data(\n",
    "        osha_data, \n",
    "        phys_chem_cols, \n",
    "        naics_col, \n",
    "        target_col\n",
    "    ):\n",
    "    '''\n",
    "    Prepare regression data for detected samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    osha_data : pd.DataFrame\n",
    "        The OSHA dataset, which may be the train or test set.\n",
    "    phys_chem_cols : list\n",
    "        List of columns representing physical-chemical properties.\n",
    "    naics_col : str\n",
    "        Column name representing NAICS code.\n",
    "    target_col : str\n",
    "        Column name representing the target variable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Features and target data.\n",
    "    '''\n",
    "    X = prepare_features(osha_data, phys_chem_cols, naics_col)\n",
    "\n",
    "    # Filter out non-detect values\n",
    "    where_detected = osha_data['detected'] == 1\n",
    "    y = np.log10(osha_data.loc[where_detected, target_col])\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def prepare_features(osha_data, phys_chem_cols, naics_col):\n",
    "    '''\n",
    "    Prepare features data including physical-chemical and NAICS features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    osha_data : pd.DataFrame\n",
    "        The OSHA dataset, which may be the train or test set.\n",
    "    phys_chem_cols : list\n",
    "        List of columns representing physical-chemical properties.\n",
    "    naics_col : str\n",
    "        Column name representing NAICS code.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Prepared features data.\n",
    "    '''\n",
    "    X_phys_chem = osha_data[phys_chem_cols]\n",
    "    X_naics = prepare_naics_features(osha_data, naics_col)\n",
    "    return pd.concat([X_phys_chem, X_naics], axis=1)\n",
    "    \n",
    "def prepare_naics_features(osha_data, naics_col):\n",
    "    '''\n",
    "    Prepare NAICS features.\n",
    "\n",
    "    For now, this is just the subsector codes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    osha_data : pd.DataFrame\n",
    "        The OSHA dataset, which may be the train or test set.\n",
    "    naics_col : str\n",
    "        Column name representing NAICS code.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Prepared NAICS features.\n",
    "    '''\n",
    "    return osha_data[naics_col].str[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e710c-5a71-4b34-999c-0b1e302b0627",
   "metadata": {},
   "source": [
    "## Duplicate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae11df98-ae8f-40d8-a32b-362b23953431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_duplicates(dataframe):\n",
    "    '''\n",
    "    Return the percentage of duplicate rows in a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The dataframe to analyze for duplicates.\n",
    "    '''\n",
    "    total_duplicates = dataframe.duplicated().sum()\n",
    "\n",
    "    proportion_duplicates = total_duplicates / len(dataframe)\n",
    "\n",
    "    return proportion_duplicates * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1463e7a6-b6b6-4488-a2e4-c80900c98430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13% of all samples are duplicates\n"
     ]
    }
   ],
   "source": [
    "print(f'{round(analyze_duplicates(osha_full))}% of all samples are duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a474a6ce-8ce7-4222-9d6b-5b3b6483631d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inspection_number</th>\n",
       "      <th>establishment_name</th>\n",
       "      <th>preferred_name</th>\n",
       "      <th>naics_unified</th>\n",
       "      <th>sector_name</th>\n",
       "      <th>subsector_name</th>\n",
       "      <th>industry_group_name</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>conc_mgm3</th>\n",
       "      <th>detected</th>\n",
       "      <th>...</th>\n",
       "      <th>bp x logoh</th>\n",
       "      <th>bp x logkoc</th>\n",
       "      <th>loghl x rt</th>\n",
       "      <th>loghl x logoh</th>\n",
       "      <th>loghl x logkoc</th>\n",
       "      <th>rt x logoh</th>\n",
       "      <th>rt x logkoc</th>\n",
       "      <th>logoh x logkoc</th>\n",
       "      <th>index_s</th>\n",
       "      <th>index_ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>HIGHWAY DEPARTMENT TOWN ROTTERDAM</td>\n",
       "      <td>1,6-Diisocyanatohexane</td>\n",
       "      <td>611110</td>\n",
       "      <td>Educational Services</td>\n",
       "      <td>Educational Services</td>\n",
       "      <td>Elementary and Secondary Schools</td>\n",
       "      <td>P</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12756</td>\n",
       "      <td>-0.200765</td>\n",
       "      <td>-0.051483</td>\n",
       "      <td>-0.077787</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.274959</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>HIGHWAY DEPARTMENT TOWN ROTTERDAM</td>\n",
       "      <td>1,6-Diisocyanatohexane</td>\n",
       "      <td>611110</td>\n",
       "      <td>Educational Services</td>\n",
       "      <td>Educational Services</td>\n",
       "      <td>Elementary and Secondary Schools</td>\n",
       "      <td>P</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12756</td>\n",
       "      <td>-0.200765</td>\n",
       "      <td>-0.051483</td>\n",
       "      <td>-0.077787</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.274959</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  inspection_number                 establishment_name  \\\n",
       "0                42  HIGHWAY DEPARTMENT TOWN ROTTERDAM   \n",
       "1                42  HIGHWAY DEPARTMENT TOWN ROTTERDAM   \n",
       "\n",
       "           preferred_name naics_unified           sector_name  \\\n",
       "0  1,6-Diisocyanatohexane        611110  Educational Services   \n",
       "1  1,6-Diisocyanatohexane        611110  Educational Services   \n",
       "\n",
       "         subsector_name               industry_group_name sample_type  \\\n",
       "0  Educational Services  Elementary and Secondary Schools           P   \n",
       "1  Educational Services  Elementary and Secondary Schools           P   \n",
       "\n",
       "   conc_mgm3  detected  ...  bp x logoh  bp x logkoc  loghl x rt  \\\n",
       "0      0.071         1  ...     0.12756    -0.200765   -0.051483   \n",
       "1      0.071         1  ...     0.12756    -0.200765   -0.051483   \n",
       "\n",
       "   loghl x logoh  loghl x logkoc  rt x logoh rt x logkoc logoh x logkoc  \\\n",
       "0      -0.077787        0.122428    0.115624    -0.18198      -0.274959   \n",
       "1      -0.077787        0.122428    0.115624    -0.18198      -0.274959   \n",
       "\n",
       "  index_s index_ss  \n",
       "0       5       11  \n",
       "1       5       11  \n",
       "\n",
       "[2 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osha_full.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074829f2-79aa-4778-9da9-a2a4e7bb2803",
   "metadata": {},
   "source": [
    "## Identically-Valued Samples\n",
    "This issue arises partly from duplication errors and partly from the method of data aggregation, which grouped samples by inspection number rather than by unique chemical or unique chemical-subsector combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24515a6a-58fe-4c14-bf37-71b4f0ce8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87% of classification training samples are identical\n"
     ]
    }
   ],
   "source": [
    "# Define helper function for convenience\n",
    "combine_Xy = lambda X, y: pd.concat([X, y], axis=1)\n",
    "\n",
    "Xy_train_class = combine_Xy(*prepare_classification_data(\n",
    "    osha_train,\n",
    "    data_config['phys_chem_cols'], \n",
    "    data_config['naics_col'],\n",
    "    data_config['class_target_col']\n",
    "))\n",
    "print(f'{round(analyze_duplicates(Xy_train_class))}% of classification training samples are identical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3924bc9-57bb-4b75-8d09-5b12caaea5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logp_pred</th>\n",
       "      <th>bp_pred</th>\n",
       "      <th>loghl_pred</th>\n",
       "      <th>rt_pred</th>\n",
       "      <th>logoh_pred</th>\n",
       "      <th>logkoc_pred</th>\n",
       "      <th>naics_unified</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.536572</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>-0.186106</td>\n",
       "      <td>0.276632</td>\n",
       "      <td>0.417972</td>\n",
       "      <td>-0.657841</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.536572</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>-0.186106</td>\n",
       "      <td>0.276632</td>\n",
       "      <td>0.417972</td>\n",
       "      <td>-0.657841</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.109881</td>\n",
       "      <td>-1.096611</td>\n",
       "      <td>1.223598</td>\n",
       "      <td>-0.985595</td>\n",
       "      <td>-0.655160</td>\n",
       "      <td>-0.398183</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.418435</td>\n",
       "      <td>-0.053236</td>\n",
       "      <td>-1.456628</td>\n",
       "      <td>-0.985595</td>\n",
       "      <td>0.271104</td>\n",
       "      <td>-1.281596</td>\n",
       "      <td>611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.220900</td>\n",
       "      <td>1.005122</td>\n",
       "      <td>-0.918491</td>\n",
       "      <td>3.024304</td>\n",
       "      <td>0.623042</td>\n",
       "      <td>1.073490</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41385</th>\n",
       "      <td>-0.039618</td>\n",
       "      <td>-0.560311</td>\n",
       "      <td>0.621655</td>\n",
       "      <td>-0.182988</td>\n",
       "      <td>0.232836</td>\n",
       "      <td>-0.475266</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41386</th>\n",
       "      <td>-0.804940</td>\n",
       "      <td>-1.202590</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>-0.985595</td>\n",
       "      <td>-0.670397</td>\n",
       "      <td>-0.798232</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41387</th>\n",
       "      <td>0.334963</td>\n",
       "      <td>-0.844608</td>\n",
       "      <td>1.277709</td>\n",
       "      <td>-0.292999</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>-0.162848</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41388</th>\n",
       "      <td>-0.536572</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>-0.186106</td>\n",
       "      <td>0.276632</td>\n",
       "      <td>0.417972</td>\n",
       "      <td>-0.657841</td>\n",
       "      <td>926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41389</th>\n",
       "      <td>-0.536572</td>\n",
       "      <td>0.305187</td>\n",
       "      <td>-0.186106</td>\n",
       "      <td>0.276632</td>\n",
       "      <td>0.417972</td>\n",
       "      <td>-0.657841</td>\n",
       "      <td>926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41390 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       logp_pred   bp_pred  loghl_pred   rt_pred  logoh_pred  logkoc_pred  \\\n",
       "0      -0.536572  0.305187   -0.186106  0.276632    0.417972    -0.657841   \n",
       "1      -0.536572  0.305187   -0.186106  0.276632    0.417972    -0.657841   \n",
       "2       0.109881 -1.096611    1.223598 -0.985595   -0.655160    -0.398183   \n",
       "3      -1.418435 -0.053236   -1.456628 -0.985595    0.271104    -1.281596   \n",
       "4       0.220900  1.005122   -0.918491  3.024304    0.623042     1.073490   \n",
       "...          ...       ...         ...       ...         ...          ...   \n",
       "41385  -0.039618 -0.560311    0.621655 -0.182988    0.232836    -0.475266   \n",
       "41386  -0.804940 -1.202590    0.510407 -0.985595   -0.670397    -0.798232   \n",
       "41387   0.334963 -0.844608    1.277709 -0.292999    0.020578    -0.162848   \n",
       "41388  -0.536572  0.305187   -0.186106  0.276632    0.417972    -0.657841   \n",
       "41389  -0.536572  0.305187   -0.186106  0.276632    0.417972    -0.657841   \n",
       "\n",
       "      naics_unified  detected  \n",
       "0               611         1  \n",
       "1               611         1  \n",
       "2               611         1  \n",
       "3               611         0  \n",
       "4               611         1  \n",
       "...             ...       ...  \n",
       "41385           332         1  \n",
       "41386           332         1  \n",
       "41387           332         1  \n",
       "41388           926         0  \n",
       "41389           926         0  \n",
       "\n",
       "[41390 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eeb91dc-34d4-46d2-8ae6-4a8ef2491d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88% of classification test samples are identical\n"
     ]
    }
   ],
   "source": [
    "Xy_test_class = combine_Xy(*prepare_classification_data(\n",
    "    osha_test,\n",
    "    data_config['phys_chem_cols'], \n",
    "    data_config['naics_col'],\n",
    "    data_config['class_target_col']\n",
    "))\n",
    "print(f'{round(analyze_duplicates(Xy_test_class))}% of classification test samples are identical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078425a8-b974-4e24-86ee-c58b03682605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14% of regression training samples are identical\n"
     ]
    }
   ],
   "source": [
    "Xy_train_reg = combine_Xy(*prepare_regression_data(\n",
    "    osha_train,\n",
    "    data_config['phys_chem_cols'], \n",
    "    data_config['naics_col'],\n",
    "    data_config['reg_target_col']\n",
    "))\n",
    "print(f'{round(analyze_duplicates(Xy_train_reg))}% of regression training samples are identical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d99b15-fb0a-485a-a324-2efe3fd20a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of regression test samples are identical\n"
     ]
    }
   ],
   "source": [
    "Xy_test_reg = combine_Xy(*prepare_regression_data(\n",
    "    osha_test,\n",
    "    data_config['phys_chem_cols'], \n",
    "    data_config['naics_col'],\n",
    "    data_config['reg_target_col']\n",
    "))\n",
    "print(f'{round(analyze_duplicates(Xy_test_reg))}% of regression test samples are identical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3f843-38ed-4cc9-a34c-64cc12a85a93",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "By inspecting the original code, not shown here, I noticed that preprocessing steps like centering and scaling were conducted on the entire dataset prior to splitting into training and test sets. This inadvertently incorporated test set information into the model training process.\n",
    "\n",
    "The code below checks for any samples that are present in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab2cbfc-fff3-408e-8e6d-5dd3e78f31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_overlapping_samples(train_df, test_df):\n",
    "    '''\n",
    "    Check for overlapping samples between training and test sets. \n",
    "    \n",
    "    A single identifier is created by joining all column values as strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataset containing features and target.\n",
    "    test_df : pd.DataFrame\n",
    "        Test dataset containing features and target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of overlapping samples between the training and test sets.\n",
    "    '''\n",
    "    train_df, test_df = train_df.copy(), test_df.copy()\n",
    "    \n",
    "    train_df['combined'] = train_df.apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    test_df['combined'] = test_df.apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    train_set = set(train_df['combined'])\n",
    "    test_set = set(test_df['combined'])\n",
    "\n",
    "    overlap = test_set.intersection(train_set)\n",
    "\n",
    "    return len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c08fc3-c143-4d4f-9eb0-af32be9c7b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of classification test samples included in training set\n"
     ]
    }
   ],
   "source": [
    "overlap_class = count_overlapping_samples(Xy_train_class, Xy_test_class) / len(Xy_test_class) * 100\n",
    "print(f'{round(overlap_class)}% of classification test samples included in training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d597b39-46d4-46fd-ae42-3e452e01988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of regression test samples included in training set\n"
     ]
    }
   ],
   "source": [
    "overlap_reg = count_overlapping_samples(Xy_train_reg, Xy_test_reg) / len(Xy_test_reg) * 100\n",
    "print(f'{round(overlap_reg)}% of regression test samples included in training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550252c-85e0-4c63-99df-5f45d8dc6fc5",
   "metadata": {},
   "source": [
    "## Potential Path Forward\n",
    "\n",
    "This last section outlines an approach to address both the immediate concerns identified and potential enhancements for a future publication.\n",
    "\n",
    "### Resolving Data Preparation Errors (Original Paper)\n",
    "1. **Address Duplicate Samples and Improper Preprocessing**: Correct the origins of duplicate samples and ensure that preprocessing steps like centering and scaling are applied appropriately after data splitting.\n",
    "2. **Re-execute the Original Bayesian Model**: After addressing the data issues, rerun the original model to assess if the previously reported performances were affected.\n",
    "3. **Consider Issuing an Erratum**: If the re-evaluation yields significantly different results, consider issuing an erratum to the original publication to correct the scientific record.\n",
    "\n",
    "### Additional Enhancements (Future Paper)\n",
    "1. **Implement More Rigorous Data Cleaning**: Adopt the more rigorous data cleaning methodology suggested by Lavoué et al., 2023, to ensure the integrity of the dataset.\n",
    "2. **Incorporate Additional OSHA Datasets**: Expand the dataset to include more recent inspection years and the IMIS dataset, enhancing the model's robustness and relevance.\n",
    "3. **Aggregate Samples by Unique Chemical or Chemical-Subsector Combination**: This strategy mitigates the issue with identically-valued samples and aligns with the intended use of the model.\n",
    "4. **Implement Cross-Validation**: Employ cross-validation techniques to more fully gauge the model's prediction error, moving beyond a single train-test split.\n",
    "5. **Evaluate Less Computationally Intensive Models**: Explore simpler models, such random forests, which may offer sufficient performance with reduced computational demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e9c76-0795-4657-b48e-6aa9a5eb8670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiu-lab",
   "language": "python",
   "name": "chiu-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
